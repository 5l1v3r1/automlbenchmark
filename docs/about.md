---
title: About
layout: category
sidebar_sort_order: 5
---

## Goals

The goal is to keep the benchmark up-to-date by rerunning evaluations when frameworks get (major) updates<sup>1</sup>.
To discourage overtuning for benchmark results, we plan to update the selection of datasets regularly<sup>2</sup>.

Currently, we limit the datasets to involve single-label classification problems on i.i.d. tabular data.
In the future, we would like to extend the types of tasks to include e.g. regression, multi-label classification and temporal data.

## Open Science
Open science is important to us.
This is a transparent benchmark: no favorites, no cheating<sup>3</sup>.
We require that all evaluated AutoML systems are open-source and all data to be freely available on [OpenML](https://www.openml.org/).
All code to rerun the benchmark is open-source.
We are open to new AutoML systems.

---
<sup>1</sup> Due to the high (computational) cost involved, we need to find a balance here.

<sup>2</sup> Frequency to be established, partially because of footnote 1.

<sup>3</sup> Word on meta-learning (could involve benchmark data)?